{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Config:** ","metadata":{}},{"cell_type":"code","source":"# NOTE: paths are Kaggle-style; change BASE/INPUT if running locally.\n\nimport os\nfrom pathlib import Path\n\n# ==== detect environment ====\nIS_KAGGLE = os.path.exists(\"/kaggle\")\n\n# ==== base dirs ====\nBASE = Path(\"/kaggle/working\") if IS_KAGGLE else Path(\"./working\")     \nINPUT = Path(\"/kaggle/input\") if IS_KAGGLE else Path(\"./input\")        \n\n# ==== repo / checkpoints / exports ====\nREPO = BASE / \"cyclegan_repo\"\nCKPTDIR = BASE / \"checkpoints_local\"\nRUN_DIR = CKPTDIR / \"my_cyclegan\"\nEXPORT_DIR = BASE / \"export_my_cyclegan_latest\"\nRESULTS = BASE / \"results\"\nLOG = BASE / \"train_live.log\"\n\n# ==== kaggle dataset (for pushing checkpoints) ====\nOWNER = \"shiristern\"                             # user/owner name in kaggle\nSLUG = \"checkpoints\"                             # dataset name for saving data\nID = f\"{OWNER}/{SLUG}\"\nSRC = INPUT / \"checkpoints\"                      # where to copy checkpoints FROM\n\n# ==== main CycleGAN dataset (A/B) ====\nDATAROOT = INPUT / \"cyclegan-mydata\" / \"content\" / \"drive\" / \"MyDrive\" / \"cyclegan_datasets\" / \"mydata\"\nTEST_A = DATAROOT / \"testA\"                      # you mentioned this\nTEST_B = DATAROOT / \"testB\"                      # and this\nREAL_A = TEST_A                                  # for FID/KID, real A\nREAL_B = TEST_B                                  # for FID/KID, real B\n\n# ==== run params ====\nNAME = \"my_cyclegan\"                             # must match folder under checkpoints_local\nEPOCH = \"250\"                                    # or \"latest\" / \"100\" ...\n\n# ==== generated images (CycleGAN standard output) ====\n# example: /kaggle/working/results/my_cyclegan/test_250/images\nGEN_DIR = RESULTS / NAME / f\"test_{EPOCH}\" / \"images\"\n\n# ==== single-image test ====\nIN_IMG = INPUT / \"test-image-cyclegan\" / \"image9.png\"             # input image for single test\nWORK_IMG = BASE / \"single\" / \"image9.png\"                         # copy here for debugging\nROOT_SINGLE = BASE / \"_single_BtoA\"                               # minimal dataroot for single run\nSINGLE_RESULTS = BASE / \"single_results\"                          # where single-image outputs go\nDIR = SINGLE_RESULTS / NAME / f\"test_{EPOCH}\" / \"images\"          # <- you asked about this\n\n# ==== FID/KID target folders ====\nOUT_FAKE_B = BASE / \"fid_AtoB_fakeB\"                              # A->B: outputs of type fake_B\nOUT_FAKE_A = BASE / \"fid_BtoA_fakeA\"                              # B->A: outputs of type fake_A\n\n# ==== checkpoints produced elsewhere (your extra paths) ====\n# G(A) -> looks like B\nG_A = INPUT / \"my-checkpoints\" / f\"epoch_{EPOCH}\" / \"fake_B\"\n# F(B) -> looks like A\nF_B = INPUT / \"my-checkpoints\" / f\"epoch_{EPOCH}\" / \"fake_A\"\n\n# ==== binary-classifier / real-vs-fake dataset root ====\nROOT = BASE / \"real_vs_fake_jpeg256\"                 # this was your ROOT = Path(...)\nBEST_PATH = BASE / \"best_resnet18_real_vs_fake.pt\"\n\n# ====== export to env so %%bash cells can use it ======\nto_export = {\n    \"BASE\": BASE,\n    \"INPUT\": INPUT,\n    \"REPO\": REPO,\n    \"CKPTDIR\": CKPTDIR,\n    \"RUN_DIR\": RUN_DIR,\n    \"EXPORT_DIR\": EXPORT_DIR,\n    \"RESULTS\": RESULTS,\n    \"LOG\": LOG,\n    \"OWNER\": OWNER,\n    \"SLUG\": SLUG,\n    \"ID\": ID,\n    \"SRC\": SRC,\n    \"DATAROOT\": DATAROOT,\n    \"TEST_A\": TEST_A,\n    \"TEST_B\": TEST_B,\n    \"REAL_A\": REAL_A,\n    \"REAL_B\": REAL_B,\n    \"NAME\": NAME,\n    \"EPOCH\": EPOCH,\n    \"GEN_DIR\": GEN_DIR,\n    \"IN_IMG\": IN_IMG,\n    \"WORK_IMG\": WORK_IMG,\n    \"ROOT_SINGLE\": ROOT_SINGLE,\n    \"SINGLE_RESULTS\": SINGLE_RESULTS,\n    \"DIR\": DIR,\n    \"OUT_FAKE_B\": OUT_FAKE_B,\n    \"OUT_FAKE_A\": OUT_FAKE_A,\n    \"G_A\": G_A,\n    \"F_B\": F_B,\n    \"REAL_VS_FAKE_ROOT\": REAL_VS_FAKE_ROOT,\n    \"BEST_PATH\": BEST_PATH,\n}\n\nfor k, v in to_export.items():\n    os.environ[k] = str(v)\n\nprint(\"âœ… CONFIG loaded.\")\nprint(\"BASE     :\", os.environ[\"BASE\"])\nprint(\"DATAROOT :\", os.environ[\"DATAROOT\"])\nprint(\"GEN_DIR  :\", os.environ[\"GEN_DIR\"])\nprint(\"TEST_A   :\", os.environ[\"TEST_A\"])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Checking:","metadata":{}},{"cell_type":"code","source":"import os\nkeys = [\n    \"BASE\", \"INPUT\", \"DATAROOT\",\n    \"GEN_DIR\", \"OUT_FAKE_A\", \"OUT_FAKE_B\",\n    \"TEST_A\", \"TEST_B\",\n    \"G_A\", \"F_B\",\n    \"SINGLE_RESULTS\", \"DIR\",\n]\nfor k in keys:\n    print(k, \"â†’\", os.environ.get(k, \"<MISSING>\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Check versions:","metadata":{}},{"cell_type":"code","source":"import torch, torchvision, platform\nprint(\"Torch:\", torch.__version__)\nprint(\"Torchvision:\", torchvision.__version__)\nprint(\"Python:\", platform.python_version())\nprint(\"Platform:\", platform.platform())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T18:11:51.203491Z","iopub.execute_input":"2025-10-30T18:11:51.203791Z","execution_failed":"2025-10-30T18:11:53.777Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Stop running processes:","metadata":{}},{"cell_type":"code","source":"%%bash\npkill -f \"/cyclegan_repo/train.py\" 2>/dev/null || true\npkill -f \"kaggle datasets version\" 2>/dev/null || true","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Preparations:**","metadata":{}},{"cell_type":"markdown","source":"Copy the checkpoints folder from input to WORKING:","metadata":{}},{"cell_type":"code","source":"%%bash\nset -e                         \n\nmkdir -p \"$RUN_DIR\"\ncp -v \"$SRC\"/latest_net_*.pth \"$RUN_DIR\"/ 2>/dev/null || true\ncp -v \"$SRC\"/*_net_*.pth     \"$RUN_DIR\"/ 2>/dev/null || true\ncp -v \"$SRC\"/{loss_log.txt,train_opt.txt,iter.txt,README.txt} \"$RUN_DIR\"/ 2>/dev/null || true\n\necho \"Now in $RUN_DIR:\"\nls -lh \"$RUN_DIR\" | head -n 50","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Set the next epoch:","metadata":{}},{"cell_type":"code","source":"%%bash\nLAST=$(ls \"$RUN_DIR\"/*_net_G_A.pth 2>/dev/null | sed -n 's#.*/\\([0-9]\\+\\)_net_G_A\\.pth#\\1#p' | sort -n | tail -n1)\nif [ -z \"${LAST:-}\" ]; then NEXT=1; else NEXT=$((LAST+1)); fi\necho \"LAST epoch = ${LAST:-none}\"\necho \"NEXT epoch_count = $NEXT\" | tee ${BASE}/NEXT.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Check if a repository exists:","metadata":{}},{"cell_type":"code","source":"%%bash\nls -ld ${BASE}/cyclegan_repo || echo \"MISSING: ${BASE}/cyclegan_repo\"\necho \"searching for train.py under ${BASE}:\"\nfind ${BASE} -maxdepth 3 -type f -name train.py 2>/dev/null | sed 's#^#/##'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Reclone the repository:","metadata":{}},{"cell_type":"code","source":"%%bash\nset -e\n[ -d \"$REPO\" ] || git clone --depth 1 https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git \"$REPO\"\nls -lh \"$REPO\" | head -n 20","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Check if metadata exists:","metadata":{}},{"cell_type":"code","source":"%%bash\nmkdir -p \"$EXPORT_DIR\"\n[ -f \"$EXPORT_DIR/dataset-metadata.json\" ] && echo \"HAS_METADATA=YES\" || echo \"HAS_METADATA=NO\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Create metadata if it does not exist:","metadata":{}},{"cell_type":"code","source":"%%bash\nset -e\ncat > \"$EXPORT_DIR/dataset-metadata.json\" <<'JSON'\n{\n  \"title\": \"CycleGAN checkpoints\",\n  \"id\": \"shiristern/checkpoints\",\n  \"licenses\": [{\"name\": \"CC0-1.0\"}]\n}\nJSON\necho \"metadata ready\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Copy the token:","metadata":{}},{"cell_type":"code","source":"import os, shutil\nos.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\nshutil.copy(\"${INPUT}/kaggle-json/kaggle.json\", os.path.expanduser(\"~/.kaggle/kaggle.json\"))\nos.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\nprint(\"Token installed.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Verify that the token was copied correctly:","metadata":{}},{"cell_type":"code","source":"%%bash\nls -l ~/.kaggle/kaggle.json || echo \"no token\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Clear memory space:","metadata":{}},{"cell_type":"markdown","source":"Clean the export folder:","metadata":{}},{"cell_type":"markdown","source":"Delete unnecessary checkpoints:","metadata":{}},{"cell_type":"code","source":"%%bash\nset -e\nif [ -d \"$EXPORT_DIR\" ]; then\n  echo \"Cleaning $EXPORT_DIR (keeping dataset-metadata.json only)â€¦\"\n  find \"$EXPORT_DIR\" -mindepth 1 -maxdepth 1 ! -name 'dataset-metadata.json' -print -exec rm -rf {} +\nfi\ndf -h ${BASE}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\nset -e\necho \"Deleting iter_* checkpoints in $RUN_DIR â€¦\"\nfind \"$RUN_DIR\" -type f -name 'iter_*_net_*.pth' -print -delete\ndf -h ${BASE}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Perform additional unnecessary deletions:","metadata":{}},{"cell_type":"code","source":"import re, pathlib\nrun_dir = pathlib.Path(RUN_DIR)\nkeep_last_epochs = 2   \n\nif run_dir.is_dir():\n    keep = {p.name for p in run_dir.glob(\"latest_net_*.pth\")}\n    epoch_files = {}\n    for p in run_dir.glob(\"*_net_*.pth\"):\n        m = re.match(r\"(\\d+)_net_.*\\.pth$\", p.name)\n        if m: epoch_files.setdefault(int(m.group(1)), []).append(p)\n    epochs = sorted(epoch_files.keys())\n    to_del = []\n    if len(epochs) > keep_last_epochs:\n        for e in epochs[:-keep_last_epochs]:\n            for p in epoch_files[e]:\n                if p.name not in keep: to_del.append(p)\n    for p in to_del:\n        try: p.unlink(); print(\"deleted:\", p.name)\n        except Exception as e: print(\"skip:\", p, e)\nelse:\n    print(\"run_dir not found:\", run_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Check memory status:","metadata":{}},{"cell_type":"code","source":"%%bash\ndf -h ${BASE}\ndu -h -d1 ${BASE} | sort -h | tail -n 15","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train the model:","metadata":{}},{"cell_type":"code","source":"%%bash\nset -euo pipefail\n\nUPLOAD_EVERY_SECONDS=180                            # Upload a new version every few seconds\n\n# 0) Clean old processes\npkill -f \"/cyclegan_repo/train.py\" 2>/dev/null || true\npkill -f \"kaggle datasets version\" 2>/dev/null || true\npkill -f \"tail -F $LOG\"            2>/dev/null || true\nrm -f /tmp/train.pid /tmp/uploader.pid || true\n: > \"$LOG\"\n\n# 1) Dependency + Code\npip -q install dominate\n[ -d \"$REPO\" ] || git clone --depth 1 https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git \"$REPO\"\n\n# 2) Next epoch_count \nLAST=$(ls \"$RUN_DIR\"/*_net_G_A.pth 2>/dev/null | sed -n 's#.*/\\([0-9]\\+\\)_net_G_A\\.pth#\\1#p' | sort -n | tail -n1)\nif [ -z \"${LAST:-}\" ]; then NEXT=1; else NEXT=$((LAST+1)); fi\necho \"â†’ epoch_count=${NEXT} (last=${LAST:-none})\" | tee -a \"$LOG\"\n\n# 3) Upload one version (if there is anything to upload)\nupload_once() {\n  mkdir -p \"$EXPORT_DIR\"\n  # Do not delete dataset-metadata.json if it exists\n  find \"$EXPORT_DIR\" -maxdepth 1 -type f ! -name 'dataset-metadata.json' -delete || true\n  cp -a \"$RUN_DIR\"/latest_net_*.pth \"$EXPORT_DIR\"/ 2>/dev/null || true\n  cp -a \"$RUN_DIR\"/*_net_*.pth    \"$EXPORT_DIR\"/ 2>/dev/null || true\n  cp -a \"$RUN_DIR\"/train_opt.txt  \"$EXPORT_DIR\"/ 2>/dev/null || true\n  cp -a \"$RUN_DIR\"/iter.txt       \"$EXPORT_DIR\"/ 2>/dev/null || true\n  cp -a \"$RUN_DIR\"/loss_log.txt   \"$EXPORT_DIR\"/ 2>/dev/null || true\n  [ -f \"$EXPORT_DIR/README.txt\" ] || echo \"CycleGAN checkpoints\" > \"$EXPORT_DIR/README.txt\"\n  # If dataset-metadata.json does not exist yet, skip (the dataset creation cell will generate it)\n  [ -f \"$EXPORT_DIR/dataset-metadata.json\" ] || return 0\n  [ -n \"$(ls -A \"$EXPORT_DIR\" | grep -v '^dataset-metadata.json$' || true)\" ] || return 0\n  kaggle datasets version -p \"$EXPORT_DIR\" -m \"auto $(date +%F_%T)\" -r tar || true\n}\n\n# 4) Loop to upload versions continuously\nuploader_loop() {\n  while true; do\n    upload_once\n    sleep \"$UPLOAD_EVERY_SECONDS\"\n  done\n}\n\n# 5) Run training in the background with continuous log flushing\n( stdbuf -oL -eL env PYTHONUNBUFFERED=1 \\\n  python -u \"$REPO/train.py\" \\\n    --dataroot \"$DATAROOT\" \\\n    --name my_cyclegan \\\n    --checkpoints_dir \"$CKPTDIR\" \\\n    --model cycle_gan \\\n    --continue_train \\\n    --epoch_count \"$NEXT\" \\\n    --n_epochs 100 --n_epochs_decay 100 \\\n    --batch_size 1 \\\n    --preprocess resize_and_crop --load_size 286 --crop_size 256 \\\n    --lambda_identity 0.5 \\\n    --save_epoch_freq 5 --save_by_iter --save_latest_freq 3000 \\\n    --print_freq 25 --num_threads 0 --no_html --verbose \\\n  2>&1 ) >> \"$LOG\" & echo $! > /tmp/train.pid\n\n# 6) Upload versions in the background + live tail of the output\nuploader_loop & echo $! > /tmp/uploader.pid\n\nfinish() {\n  echo \"[uploader] final upload & cleanup\" | tee -a \"$LOG\"\n  upload_once\n  if [ -f /tmp/uploader.pid ]; then kill \"$(cat /tmp/uploader.pid)\" 2>/dev/null || true; rm -f /tmp/uploader.pid; fi\n}\ntrap finish EXIT\n\necho \"ðŸƒ Training is running (PID $(cat /tmp/train.pid)). Showing live output:\" | tee -a \"$LOG\"\nexec tail -F \"$LOG\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test images check:","metadata":{}},{"cell_type":"code","source":"%%bash\n# Run test images\nset -e\n\n# 0) Ensure the repo is in the location the script expects\nif [ ! -d ${BASE}/cyclegan_repo ]; then\n  if [ -d ${BASE}/pytorch-CycleGAN-and-pix2pix ]; then\n    ln -s ${BASE}/pytorch-CycleGAN-and-pix2pix ${BASE}/cyclegan_repo\n  else\n    git clone --depth 1 https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git ${BASE}/cyclegan_repo\n  fi\nfi\n\n# 1) Verify that test.py actually exists\n[ -f \"$REPO/test.py\" ] || { echo \"ERROR: missing $REPO/test.py\"; echo \"Found instead:\"; find ${BASE} -maxdepth 3 -name test.py; exit 1; }\n\n# 3) Minimal dependencies for test\npip -q install dominate pillow numpy >/dev/null\n\n# 4) Run test Aâ†’B\npython -u \"$REPO/test.py\" \\\n  --dataroot \"$DATAROOT\" \\\n  --name \"$NAME\" \\\n  --checkpoints_dir \"$CKPTDIR\" \\\n  --model cycle_gan \\\n  --phase test \\\n  --epoch \"$EPOCH\" \\\n  --direction AtoB \\\n  --num_test 100000 \\\n  --results_dir \"$RESULTS\" \\\n  --eval\n\n# 5) Run test Bâ†’A\npython -u \"$REPO/test.py\" \\\n  --dataroot \"$DATAROOT\" \\\n  --name \"$NAME\" \\\n  --checkpoints_dir \"$CKPTDIR\" \\\n  --model cycle_gan \\\n  --phase test \\\n  --epoch \"$EPOCH\" \\\n  --direction BtoA \\\n  --num_test 100000 \\\n  --results_dir \"$RESULTS\" \\\n  --eval\n\necho \"Saved under: ${RESULTS}/${NAME}/test_${EPOCH}/images\"\nls -lh \"${RESULTS}/${NAME}/test_${EPOCH}/images\" | head -n 30 || true","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Zip the results folder:","metadata":{}},{"cell_type":"code","source":"%%bash\ncd ${BASE}/checkpoints_local\nzip -r test_latest_images.zip my_cyclegan","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Run the model on a specific image:","metadata":{}},{"cell_type":"code","source":"%%bash\nset -e\n\n# 0) Verify repo and basic dependencies\n[ -d \"$REPO\" ] || git clone --depth 1 https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git \"$REPO\" >/dev/null\npip -q install dominate pillow numpy >/dev/null\n\n# 1) Copy the image to working (for easier debugging)\nmkdir -p \"$(dirname \"$WORK_IMG\")\"\ncp -v \"$IN_IMG\" \"$WORK_IMG\"\n\n# 2) Build a minimal dataroot with two folders (required by UnalignedDataset)\nrm -rf \"$ROOT_SINLGE\"\nmkdir -p \"$ROOT_SINLGE/testA\" \"$ROOT_SINLGE/testB\"\n\n# Direction Bâ†’A: place the image in testB; put a dummy file in testA so itâ€™s not empty\ncp -v \"$WORK_IMG\" \"$ROOT_SINLGE/testB/one.jpg\"\ncp -v \"$WORK_IMG\" \"$ROOT_SINLGE/testA/dummy.jpg\"\n\n# 3) Run test.py\npython -u \"$REPO/test.py\" \\\n  --dataroot \"$ROOT_SINLGE\" \\\n  --name \"$NAME\" \\\n  --checkpoints_dir \"$CKPTDIR\" \\\n  --model cycle_gan \\\n  --phase test \\\n  --epoch \"$EPOCH\" \\\n  --direction BtoA \\\n  --num_test 1 \\\n  --results_dir \"$RESULTS\" \\\n  --eval --serial_batches\n\n# 4) Show where the results were saved and how many files were generated\nOUTDIR=\"${SINGLE_RESULTS}/${NAME}/test_${EPOCH}/images\"\necho \"âœ… Saved to: ${OUTDIR}\"\nls -lh \"${OUTDIR}\" | head -n 30\necho \"Total files:\" $(ls -1 \"${OUTDIR}\" | wc -l)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Display the generated image next to the input image:","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport math\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Config\nEXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".tif\", \".tiff\", \".webp\"}  \nPAGE = 1          \nPER_PAGE = 48     \nCOLS = 6          \nCELL_SIZE = 3     \n\n# Collect files\npaths = [p for p in Path(DIR).rglob(\"*\") if p.suffix.lower() in EXTS]\npaths.sort()\ntotal = len(paths)\nif total == 0:\n    print(f\"No images found in path: {DIR}\")\nelse:\n    start = (PAGE - 1) * PER_PAGE\n    end = min(start + PER_PAGE, total)\n    subset = paths[start:end]\n    rows = math.ceil(len(subset) / COLS) or 1\n\n    # Plotting\n    plt.figure(figsize=(COLS * CELL_SIZE, rows * CELL_SIZE))\n    for i, p in enumerate(subset, 1):\n        try:\n            img = Image.open(p)\n            plt.subplot(rows, COLS, i)\n            plt.imshow(img)\n            plt.axis(\"off\")\n            plt.title(p.name, fontsize=8)\n        except Exception as e:\n            print(f\"Skipping {p}: {e}\")\n\n    plt.tight_layout()\n    plt.show()\n   print(f\"Showing {len(subset)} of {total} images from folder: {DIR} | Page {PAGE} ({start+1}â€“{end})\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluate metrics:","metadata":{}},{"cell_type":"markdown","source":"Copy images to the desired folder for metrics evaluation:","metadata":{}},{"cell_type":"code","source":"%%bash\nset -euo pipefail\n\n# Preparation\nif [ ! -d \"$GEN_DIR\" ]; then\n      echo \"âŒ No results directory found: $GEN_DIR\"\n  exit 1\nfi\n\necho \"Cleaning and creating target foldersâ€¦\"\nrm -rf \"$OUT_FAKE_B\" \"$OUT_FAKE_A\"\nmkdir -p \"$OUT_FAKE_B\" \"$OUT_FAKE_A\"\n\n# Copy: fake_B and fake_A\necho \"Copying fake_B â†’ $OUT_FAKE_B\"\nfind \"$GEN_DIR\" -maxdepth 1 -type f \\( -iname \"*_fake_B.png\" -o -iname \"*_fake_B.jpg\" -o -iname \"*_fake_B.jpeg\" \\) -exec cp -t \"$OUT_FAKE_B\" {} +\necho \"Copying fake_A â†’ $OUT_FAKE_A\"\nfind \"$GEN_DIR\" -maxdepth 1 -type f \\( -iname \"*_fake_A.png\" -o -iname \"*_fake_A.jpg\" -o -iname \"*_fake_A.jpeg\" \\) -exec cp -t \"$OUT_FAKE_A\" {} +\n\n# Short report\ncB=$(find \"$OUT_FAKE_B\" -type f | wc -l)\ncA=$(find \"$OUT_FAKE_A\" -type f | wc -l)\necho \"âœ… Copied: fake_B=$cB, fake_A=$cA\"\necho \"Sample from fake_B folder:\"\nls -lh \"$OUT_FAKE_B\" | head -n 10 || true\necho \"Sample from fake_A folder:\"\nls -lh \"$OUT_FAKE_A\" | head -n 10 || true","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"FID metric evaluation:","metadata":{}},{"cell_type":"code","source":"%%bash\nset -e\npip -q install pytorch-fid\n\nrm -rf \"$OUT_FAKE_B\" \"$OUT_FAKE_A\"\nmkdir -p \"$OUT_FAKE_B\" \"$OUT_FAKE_A\"\n\nfind \"$GEN_DIR\" -maxdepth 1 -type f -name \"*_fake_B.png\" -exec cp {} \"$OUT_FAKE_B/\" \\;\nfind \"$GEN_DIR\" -maxdepth 1 -type f -name \"*_fake_A.png\" -exec cp {} \"$OUT_FAKE_A/\" \\;\n\necho \"FID A->B (real B vs fake_B):\"\npytorch-fid \"$REAL_B\" \"$OUT_FAKE_B\" | tee ${BASE}/FID_AtoB.txt\n\necho \"FID B->A (real A vs fake_A):\"\npytorch-fid \"$REAL_A\" \"$OUT_FAKE_A\" | tee ${BASE}/FID_BtoA.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"KID metric evaluation:","metadata":{}},{"cell_type":"code","source":"%%bash\nset -e\npip -q install torch-fidelity >/dev/null\n\n# Export variables so Python can access them via os.environ\nexport REAL_A\nexport REAL_B\nexport OUT_FAKE_B\nexport OUT_FAKE_A\n\n# 2) Run Python (no quotes needed in Heredoc since variables were already exported)\npython - <<PY\nimport os, glob\nfrom torch_fidelity import calculate_metrics\n\ndef count_imgs(d):\n    # Recursive search + support for uppercase extensions\n    exts = (\"**/*.png\",\"**/*.jpg\",\"**/*.jpeg\",\"**/*.PNG\",\"**/*.JPG\",\"**/*.JPEG\")\n    return sum(len(glob.glob(os.path.join(d, e), recursive=True)) for e in exts)\n\ndef extract_kid_fields(m):\n    for mean_key, std_key in [\n        (\"kid_mean\", \"kid_std\"),\n        (\"kernel_inception_distance_mean\", \"kernel_inception_distance_std\"),\n    ]:\n        if mean_key in m:\n            mean = float(m[mean_key])\n            std  = float(m.get(std_key, float('nan')))\n            return mean, std\n    raise KeyError(f\"KID not found in keys: {list(m.keys())}\")\n\ndef run_kid(gen_dir, real_dir, label):\n    n_gen  = count_imgs(gen_dir)\n    n_real = count_imgs(real_dir)\n    print(f\"[dbg] {label}: gen={n_gen}, real={n_real}, gen_dir={gen_dir}, real_dir={real_dir}\")\n    if n_gen == 0 or n_real == 0:\n        print(f\"{label}: âŒ No samples (gen={n_gen}, real={n_real})\")\n        return\n    subset = min(n_gen, n_real)  \n        input1=gen_dir, input2=real_dir,\n        cuda=True, isc=False, fid=False, kid=True,\n        kid_subset_size=subset,\n        kid_subset_num=100\n    )\n    mean, std = extract_kid_fields(m)\n    print(f\"{label}: KID_mean={mean:.6f} Â± {std:.6f} (lower=better) | subset_size={subset}, gen={n_gen}, real={n_real}\")\n\nREAL_A = os.environ[\"REAL_A\"]; REAL_B = os.environ[\"REAL_B\"]\nFAKE_A = os.environ[\"FAKE_A\"]; FAKE_B = os.environ[\"FAKE_B\"]\n\nrun_kid(FAKE_B, REAL_B, \"KID A->B (fake_B vs real_B)\")\nrun_kid(FAKE_A, REAL_A, \"KID B->A (fake_A vs real_A)\")\nPY","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"LPIPS, PSNR, SSIM metric evaluations - preperations:","metadata":{}},{"cell_type":"code","source":"%%bash\nset -e\npip -q install pillow numpy tqdm scikit-image lpips\n\ncat > ${BASE}/paired_metrics_from_cyclegan.py << 'PY'\nimport os, glob, csv\nfrom PIL import Image\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nimport lpips\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\n\nEXSTS = (\".png\",\".jpg\",\".jpeg\",\".bmp\",\".webp\")\n\ndef list_images(d):\n    return sorted([p for p in glob.glob(os.path.join(d, \"*\")) if p.lower().endswith(EXSTS)])\n\ndef stem_from_fake(path, suffix):\n    base = os.path.basename(path)\n    if base.endswith(suffix):\n        return base[: -len(suffix)]\n    if \"_fake_\" in base:\n        return base.split(\"_fake_\")[0]\n    return os.path.splitext(base)[0]\n\ndef find_real(real_dir, stem):\n    for ext in EXSTS:\n        cand = os.path.join(real_dir, stem + ext)\n        if os.path.isfile(cand):\n            return cand\n    return None\n\ndef load_rgb(path, size=None):\n    im = Image.open(path).convert(\"RGB\")\n    if size is not None:\n        im = im.resize(size, Image.BICUBIC)\n    return np.array(im)\n\ndef to_torch_img255(arr_uint8):\n    ten = torch.from_numpy(arr_uint8).permute(2,0,1).float()/255.0\n    ten = ten*2.0 - 1.0\n    return ten.unsqueeze(0)\n\ndef evaluate_pair_dir(real_dir, fake_dir, direction_tag, fake_suffix):\n    fakes = list_images(fake_dir)\n    if len(fakes)==0:\n            raise SystemExit(f\"No images found in {fake_dir}\")\n    loss_fn = lpips.LPIPS(net='alex').eval()\n    if torch.cuda.is_available():\n        loss_fn = loss_fn.cuda()\n\n    rows = []\n    psnr_vals, ssim_vals, lpips_vals = [], [], []\n\n    for fp in tqdm(fakes, desc=f\"{direction_tag}\"):\n        stem = stem_from_fake(fp, fake_suffix)\n        rp = find_real(real_dir, stem)\n        if rp is None:\n            continue\n\n        real = load_rgb(rp)\n        fake = load_rgb(fp, size=(real.shape[1], real.shape[0]))\n\n        psnr_v = psnr(real, fake, data_range=255)\n        ssim_v = ssim(real, fake, channel_axis=2, data_range=255)\n\n        r_t = to_torch_img255(real)\n        f_t = to_torch_img255(fake)\n        if torch.cuda.is_available():\n            r_t = r_t.cuda(); f_t = f_t.cuda()\n        lpips_v = loss_fn.forward(r_t, f_t).item()\n\n        rows.append([os.path.basename(fp), os.path.basename(rp), psnr_v, ssim_v, lpips_v])\n        psnr_vals.append(psnr_v); ssim_vals.append(ssim_v); lpips_vals.append(lpips_v)\n\n    if len(rows)==0:\n        raise SystemExit(\"No matching pairs found (check file names).\")\n\n    # Writing CSV file\n    out_csv = f\"${BASE}/{direction_tag}_pair_metrics.csv\"\n    with open(out_csv, \"w\", newline=\"\") as f:\n        w = csv.writer(f)\n        w.writerow([\"fake_file\",\"real_file\",\"PSNR\",\"SSIM\",\"LPIPS_alex\"])\n        w.writerows(rows)\n\n    import numpy as np\n    print(f\"{direction_tag}  Num pairs: {len(rows)}\")\n    print(f\"PSNR  mean={np.mean(psnr_vals):.4f}  std={np.std(psnr_vals):.4f}\")\n    print(f\"SSIM  mean={np.mean(ssim_vals):.4f}  std={np.std(ssim_vals):.4f}\")\n    print(f\"LPIPS mean={np.mean(lpips_vals):.4f}  std={np.std(lpips_vals):.4f}\")\n    print(f\"Saved per-image CSV -> {out_csv}\")\n\nif __name__ == \"__main__\":\n    import argparse\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--real_a\", required=True)\n    ap.add_argument(\"--real_b\", required=True)\n    ap.add_argument(\"--fake_a_dir\", required=True)\n    ap.add_argument(\"--fake_b_dir\", required=True)\n    args = ap.parse_args()\n\n    # Aâ†’B: compare fake_B with real_B\n    evaluate_pair_dir(args.real_b, args.fake_b_dir, \"AtoB\", \"_fake_B.png\")\n\n    # Bâ†’A: compare fake_A with real_A\n    evaluate_pair_dir(args.real_a, args.fake_a_dir, \"BtoA\", \"_fake_A.png\")\nPY","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Actual execution:","metadata":{}},{"cell_type":"code","source":"%%bash\nset -e\n\npython ${BASE}/paired_metrics_from_cyclegan.py \\\n  --real_a \"$REAL_A\" \\\n  --real_b \"$REAL_B\" \\\n  --fake_a_dir \"$OUT_FAKE_A\" \\\n  --fake_b_dir \"$OUT_FAKE_B\" \\\n  | tee ${BASE}/PAIR_METRICS.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Train a binary classifier to check accuracy:**","metadata":{}},{"cell_type":"markdown","source":"Build a unified dataset (JPEG 256Ã—256) from the folders you provided:","metadata":{}},{"cell_type":"code","source":"# !pip -q install pillow\nimport os, glob, shutil\nfrom pathlib import Path\nfrom PIL import Image\n\n( ROOT/\"real\").mkdir(parents=True, exist_ok=True)\n( ROOT/\"fake\").mkdir(parents=True, exist_ok=True)\n\ndef export_folder(src_dir, dst_dir, max_side=256, jpeg_q=95):\n    exts = (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.bmp\",\"*.webp\")\n    counter = 0\n    for ext in exts:\n        for p in sorted(glob.glob(os.path.join(src_dir, ext))):\n            try:\n                img = Image.open(p).convert(\"RGB\")\n                img = img.resize((max_side, max_side), Image.BICUBIC)\n                out = Path(dst_dir)/f\"{Path(src_dir).name}_{Path(p).stem}_{counter}.jpg\"\n                img.save(out, format=\"JPEG\", quality=jpeg_q, optimize=True)\n                counter += 1\n            except Exception as e:\n                pass\n    return counter\n\nn_real = export_folder(TEST_A, ROOT/\"real\") + export_folder(TEST_B, ROOT/\"real\")\nn_fake = export_folder(G_A, ROOT/\"fake\")   + export_folder(F_B, ROOT/\"fake\")\nprint(\"Exported -> real:\", n_real, \" fake:\", n_fake, \" to:\", ROOT)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Training:","metadata":{}},{"cell_type":"code","source":"import io, random, math, time\nfrom pathlib import Path\n\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms, models\n\n# Config\nIMG_SIZE = 256\nCROP = 224\nBATCH = 32\nEPOCHS = 8\nVAL_PCT = 0.15\nTEST_PCT = 0.15\nLR = 1e-4\nSEED = 42\nFREEZE_BACKBONE_EPOCHS = 0   \n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# Augmentation including random JPEG recompress\nclass RandomJPEG:\n    def __init__(self, qmin=80, qmax=95, p=0.7):\n        self.qmin, self.qmax, self.p = qmin, qmax, p\n    def __call__(self, img: Image.Image):\n        if random.random() > self.p:\n            return img\n        buf = io.BytesIO()\n        img.save(buf, format=\"JPEG\", quality=random.randint(self.qmin, self.qmax))\n        buf.seek(0)\n        return Image.open(buf).convert(\"RGB\")\n\ntrain_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    RandomJPEG(p=0.8),\n    transforms.RandomResizedCrop(CROP, scale=(0.7, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.GaussianBlur(kernel_size=3),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n])\n\neval_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.CenterCrop(CROP),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n])\n\n# Data\nfull_train_tf = train_tf\nfull_eval_tf  = eval_tf\n\nfull_ds_for_split = datasets.ImageFolder(ROOT.as_posix(), transform=full_train_tf)\nidxs = np.arange(len(full_ds_for_split))\nnp.random.shuffle(idxs)\n\nn = len(idxs)\nn_test = int(TEST_PCT * n)\nn_val  = int(VAL_PCT  * n)\nn_train= n - n_val - n_test\ntrain_idx = idxs[:n_train]\nval_idx   = idxs[n_train:n_train+n_val]\ntest_idx  = idxs[n_train+n_val:]\n\n# Important: validation/test transforms without augmentation\ntrain_ds = Subset(datasets.ImageFolder(ROOT.as_posix(), transform=full_train_tf), train_idx)\nval_ds   = Subset(datasets.ImageFolder(ROOT.as_posix(), transform=full_eval_tf),  val_idx)\ntest_ds  = Subset(datasets.ImageFolder(ROOT.as_posix(), transform=full_eval_tf),  test_idx)\n\ntrain_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=2, pin_memory=True)\nval_dl   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\ntest_dl  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n\nprint(f\"Split sizes -> train: {len(train_ds)} | val: {len(val_ds)} | test: {len(test_ds)}\")\n\n# Model\nmodel = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 1)  \nmodel = model.to(device)\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=LR)\nscaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n\n# Optional: freeze the backbone for the first few epochs\ndef set_backbone_trainable(is_trainable: bool):\n    for name, p in model.named_parameters():\n        if not name.startswith(\"fc.\"):\n            p.requires_grad = is_trainable\n\nif FREEZE_BACKBONE_EPOCHS > 0:\n    set_backbone_trainable(False)\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n\nbest_val_acc = 0.0\nbest_epoch = -1\n\n# Training Loop\nfor ep in range(1, EPOCHS + 1):\n    t0 = time.time()\n    model.train()\n    run_loss, seen = 0.0, 0\n\n    if ep == FREEZE_BACKBONE_EPOCHS + 1 and FREEZE_BACKBONE_EPOCHS > 0:\n        set_backbone_trainable(True)\n        optimizer = optim.Adam(model.parameters(), lr=LR)\n\n    for xb, yb in train_dl:\n        xb = xb.to(device, non_blocking=True)\n        yb = yb.float().unsqueeze(1).to(device, non_blocking=True)  # Shape: [B,1]\n\n        optimizer.zero_grad(set_to_none=True)\n        with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n            logits = model(xb)\n            loss = criterion(logits, yb)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        bs = xb.size(0)\n        run_loss += loss.item() * bs\n        seen += bs\n\n    train_loss = run_loss / max(seen, 1)\n\n    # Validation\n    model.eval()\n    probs, targs = [], []\n    with torch.no_grad(), torch.cuda.amp.autocast(enabled=False):\n        for xb, yb in val_dl:\n            xb = xb.to(device, non_blocking=True)\n            logits = model(xb).squeeze(1).cpu().numpy()\n            p = 1 / (1 + np.exp(-logits))  # sigmoid\n            probs.append(p)\n            targs.append(yb.numpy())\n    probs = np.concatenate(probs) if probs else np.array([])\n    targs = np.concatenate(targs) if targs else np.array([])\n    preds = (probs >= 0.5).astype(int)\n    val_acc = accuracy_score(targs, preds) if targs.size else float(\"nan\")\n\n    dt = time.time() - t0\n    print(f\"Epoch {ep}/{EPOCHS} | train_loss={train_loss:.4f} | val_acc={val_acc:.3f} | {dt:.1f}s\")\n\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        best_epoch = ep\n        torch.save(model.state_dict(), BEST_PATH)\n        print(\"** saved best model **\")\n\nprint(f\"Best val acc: {best_val_acc:.3f} (epoch {best_epoch}) | path: {BEST_PATH}\")\n\n# Evaluation on the test set \n# Load the best model before evaluation\nstate = torch.load(BEST_PATH, map_location=device)\nmodel.load_state_dict(state)\nmodel.eval()\n\nprobs, targs = [], []\nwith torch.no_grad():\n    for xb, yb in test_dl:\n        xb = xb.to(device, non_blocking=True)\n        logits = model(xb).squeeze(1).cpu().numpy()\n        p = 1 / (1 + np.exp(-logits))\n        probs.append(p)\n        targs.append(yb.numpy())\n\nprobs = np.concatenate(probs) if probs else np.array([])\ntargs = np.concatenate(targs) if targs else np.array([])\npreds = (probs >= 0.5).astype(int)\n\nacc = accuracy_score(targs, preds) if targs.size else float(\"nan\")\ncm = confusion_matrix(targs, preds) if targs.size else None\nprint(\"\\n=== Test Metrics ===\")\nprint(\"Accuracy:\", round(acc, 4))\nif cm is not None:\n    print(\"Confusion matrix:\\n\", cm)\n    print(\"\\nClassification report:\\n\", classification_report(targs, preds, digits=4))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Evaluate on test + save FP/FN samples + short Grad-CAM:","metadata":{}},{"cell_type":"code","source":"import torch, numpy as np, matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nfrom pathlib import Path\n\nmodel.load_state_dict(torch.load(\"${BASE}/best_resnet18_real_vs_fake.pt\", map_location=device))\nmodel.eval()\n\nall_probs, all_targs, all_imgs, all_paths = [], [], [], []\neval_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.CenterCrop(CROP),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n])\nraw_tf  = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)),\n                              transforms.CenterCrop(CROP),\n                              transforms.ToTensor()])\n\nclass ImageFolderWithPaths(datasets.ImageFolder):\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        img = self.loader(path)\n        input_img = eval_tf(img)\n        raw_img   = raw_tf(img)\n        return input_img, target, raw_img, path\n\ntest_ds_paths = ImageFolderWithPaths(ROOT.as_posix())\ntest_ds_paths.samples = [test_ds.dataset.samples[i] for i in test_ds.indices]  \n\ntest_dl_paths = DataLoader(test_ds_paths, batch_size=16, shuffle=False, num_workers=2)\n\nwith torch.no_grad():\n    for xb, yb, rawb, paths in test_dl_paths:\n        xb = xb.to(device)\n        p = model(xb).squeeze(1).cpu().numpy()\n        all_probs.append(p); all_targs.append(yb.numpy()); all_imgs.append(rawb); all_paths += list(paths)\n\nall_probs = np.concatenate(all_probs)\nall_targs = np.concatenate(all_targs)\nall_preds = (all_probs>=0.5).astype(int)\nacc = accuracy_score(all_targs, all_preds)\ncm  = confusion_matrix(all_targs, all_preds, labels=[0,1])\nprint(f\"\\nTest Accuracy: {acc:.3f}\")\nprint(\"Confusion Matrix [rows=true 0/1, cols=pred 0/1]:\\n\", cm)\nprint(\"\\nReport:\\n\", classification_report(all_targs, all_preds, target_names=['fake','real']))\n\nSave 10 FP/FN examples to understand model behavior\nout_dir = Path(\"${BASE}/misclassified_examples\")\n(out_dir/\"FP\").mkdir(parents=True, exist_ok=True)  # true=fake, pred=real\n(out_dir/\"FN\").mkdir(parents=True, exist_ok=True)  # true=real, pred=fake\n\nimgs = torch.cat(all_imgs, dim=0)\ndef denorm(x):\n    return x.clamp(0,1)\n\nfp_idx = np.where((all_targs==0)&(all_preds==1))[0][:10]\nfn_idx = np.where((all_targs==1)&(all_preds==0))[0][:10]\nfor i in fp_idx:\n    save_image(denorm(imgs[i]), out_dir/\"FP\"/f\"fp_{i}.png\")\nfor i in fn_idx:\n    save_image(denorm(imgs[i]), out_dir/\"FN\"/f\"fn_{i}.png\")\nprint(\"Saved FP/FN samples to:\", out_dir)\n\n# Grad-CAM on layer4\n# (simple and short: generate a heatmap for one image)\nimport torch.nn.functional as F\n\ntarget_layer = model.layer4[-1].conv2 \ngrads = None\nacts  = None\ndef save_grad(module, grad_input, grad_output):\n    global grads\n    grads = grad_output[0]\ndef save_act(module, input, output):\n    global acts\n    acts = output\n\nhook_f = target_layer.register_forward_hook(save_act)\nhook_b = target_layer.register_backward_hook(lambda m, gin, gout: save_grad(m, gin, gout))\n\n# Run Grad-CAM on the first 8 test images\ngc_dir = Path(\"${BASE}/gradcam\")\ngc_dir.mkdir(parents=True, exist_ok=True)\n\nfor idx in range(min(8, len(test_ds_paths))):\n    x, y, raw, path = test_ds_paths[idx]\n    x = x.unsqueeze(0).to(device)\n    model.zero_grad()\n    out = model(x)             # sigmoid\n    score = out[0,0]\n    score.backward()           # d score / d features\n    g = grads.cpu().numpy()[0] # (C,H,W)\n    a = acts.detach().cpu().numpy()[0]\n    weights = g.mean(axis=(1,2))           \n    cam = np.maximum((weights[:,None,None]*a).sum(axis=0), 0)\n    cam = cam / (cam.max()+1e-8)\n    # Save as a simple heatmap image (no fancy colors to keep code short)\n    import cv2, numpy as np\n    cam_img = (cam*255).astype(np.uint8)\n    cam_img = cv2.resize(cam_img, (raw.shape[2], raw.shape[1]))\n    cv2.imwrite(str(gc_dir/f\"cam_{idx}.png\"), cam_img)\n\nhook_f.remove(); hook_b.remove()\nprint(\"Saved Grad-CAM maps to:\", gc_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Display the Grad-CAM images:","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nFOLDER = Path(\"${BASE}/gradcam\")  \n\nIMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\", \".webp\", \".gif\"}\n\npaths = sorted([p for p in FOLDER.iterdir() if p.is_file() and p.suffix.lower() in IMG_EXTS])[:7]\n\nif not paths:\n    print(\"No images found in folder:\", FOLDER)\nelse:\n    n = len(paths)\n    if n >= 5:\n        rows, cols = 2, 4\n    else:\n        rows, cols = 1, n\n\n    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n    axes = axes.ravel() if isinstance(axes, (list, tuple)) or hasattr(axes, \"ravel\") else [axes]\n\n    for i, p in enumerate(paths):\n        axes[i].imshow(Image.open(p))\n        axes[i].set_title(p.name, fontsize=8)\n        axes[i].axis(\"off\")\n\n    for j in range(len(paths), rows*cols):\n        axes[j].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Display sample images:","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nFOLDER = Path(\"${BASE}/misclassified_examples/FP\")  \n\nIMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\", \".webp\", \".gif\"}\n\npaths = sorted([p for p in FOLDER.iterdir() if p.is_file() and p.suffix.lower() in IMG_EXTS])[:8]\n\nif not paths:\n    print(\"No images found in folder:\", FOLDER)\nelse:\n    n = len(paths)\n    if n >= 5:\n        rows, cols = 2, 4\n    else:\n        rows, cols = 1, n\n\n    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n    axes = axes.ravel() if isinstance(axes, (list, tuple)) or hasattr(axes, \"ravel\") else [axes]\n\n    for i, p in enumerate(paths):\n        axes[i].imshow(Image.open(p))\n        axes[i].set_title(p.name, fontsize=8)\n        axes[i].axis(\"off\")\n\n    for j in range(len(paths), rows*cols):\n        axes[j].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}