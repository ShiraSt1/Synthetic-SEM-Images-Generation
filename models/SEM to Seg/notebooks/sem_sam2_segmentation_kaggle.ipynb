{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEM SAM2 Segmentation Pipeline (Kaggle)\n",
    "This notebook was executed in the Kaggle environment  \n",
    "using `/kaggle/input` datasets and saving outputs to `/kaggle/working`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-29T22:17:00.312202Z",
     "iopub.status.busy": "2025-09-29T22:17:00.311968Z",
     "iopub.status.idle": "2025-09-29T22:17:00.322231Z",
     "shell.execute_reply": "2025-09-29T22:17:00.321543Z",
     "shell.execute_reply.started": "2025-09-29T22:17:00.312181Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images will be read from: ['/kaggle/input']\n",
      "Outputs will be written under: /kaggle/working/SEM_SAM2/output\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "\n",
    "# Local working directory in Kaggle (to store models/outputs)\n",
    "WORK_ROOT = \"/kaggle/working/SEM_SAM2\"\n",
    "DIRS = {\n",
    "    \"root\": WORK_ROOT,\n",
    "    \"models\": f\"{WORK_ROOT}/models\",   # model weights/configs will be saved here\n",
    "    \"output\": f\"{WORK_ROOT}/output\",   # overlay results and manifests will be saved here\n",
    "}\n",
    "\n",
    "for d in DIRS.values():\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Where do the images come from?\n",
    "INPUT_DIRS = [\"/kaggle/input\"]\n",
    "\n",
    "# Root for relative paths in manifest (cosmetic, for path display)\n",
    "MAP_ROOT = \"/kaggle\"\n",
    "\n",
    "print(\"Images will be read from:\", INPUT_DIRS)\n",
    "print(\"Outputs will be written under:\", DIRS[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T22:17:00.323811Z",
     "iopub.status.busy": "2025-09-29T22:17:00.323625Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip -q install opencv-python matplotlib supervision\n",
    "!pip -q install git+https://github.com/facebookresearch/segment-anything-2.git\n",
    "\n",
    "import torch, cv2, numpy as np, glob, os\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!wget -qO- https://httpbin.org/ip | head -c 80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "CFG_NAME = \"sam2_hiera_l.yaml\"\n",
    "CKPT_NAME = \"sam2_hiera_large.pt\"\n",
    "\n",
    "CFG_PATH  = os.path.join(DIRS[\"models\"], CFG_NAME)\n",
    "CKPT_PATH = os.path.join(DIRS[\"models\"], CKPT_NAME)\n",
    "\n",
    "CFG_URL  = \"https://huggingface.co/spaces/SkalskiP/segment-anything-model-2/resolve/main/configs/sam2_hiera_l.yaml\"\n",
    "CKPT_URL = \"https://huggingface.co/spaces/SkalskiP/segment-anything-model-2/resolve/main/checkpoints/sam2_hiera_large.pt\"\n",
    "\n",
    "os.makedirs(DIRS[\"models\"], exist_ok=True)\n",
    "\n",
    "def download(url, dst):\n",
    "    print(f\"Downloading -> {dst}\")\n",
    "    urllib.request.urlretrieve(url, dst)\n",
    "    sz = os.path.getsize(dst) / (1024*1024)\n",
    "    print(f\"Saved {dst} ({sz:.1f} MB)\")\n",
    "\n",
    "if not os.path.exists(CFG_PATH):\n",
    "    download(CFG_URL, CFG_PATH)\n",
    "else:\n",
    "    print(\"Config exists:\", CFG_PATH)\n",
    "\n",
    "if not os.path.exists(CKPT_PATH):\n",
    "    download(CKPT_URL, CKPT_PATH)  # ~900MB\n",
    "else:\n",
    "    print(\"Weights exist:\", CKPT_PATH)\n",
    "\n",
    "print(\"Ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SAM2_PARAMS = dict(\n",
    "    points_per_side=32,\n",
    "    points_per_batch=64,\n",
    "    pred_iou_thresh=0.7,\n",
    "    stability_score_thresh=0.92,\n",
    "    stability_score_offset=0.7,\n",
    "    crop_n_layers=1,\n",
    "    box_nms_thresh=0.7,\n",
    ")\n",
    "\n",
    "ALPHA = 0.45\n",
    "MAX_IMAGES = None   \n",
    "print(\"SAM2 params:\", SAM2_PARAMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np, cv2\n",
    "\n",
    "def to_rgb(img):\n",
    "    if img.ndim == 2:\n",
    "        return cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def colorize_instances_sam2(masks, h, w):\n",
    "    overlay = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    n = max(len(masks), 1)\n",
    "    for i, m in enumerate(masks):\n",
    "        seg = m[\"segmentation\"]\n",
    "        hue = int(180 * i / n)\n",
    "        color_hsv = np.uint8([[[hue, 200, 255]]])\n",
    "        color_bgr = cv2.cvtColor(color_hsv, cv2.COLOR_HSV2BGR)[0,0,:]\n",
    "        color_rgb = color_bgr[::-1]\n",
    "        overlay[seg] = color_rgb\n",
    "    return overlay\n",
    "\n",
    "def blend_overlay(rgb, overlay_rgb, alpha=0.45):\n",
    "    return (rgb * (1 - alpha) + overlay_rgb * alpha).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "import sam2, shutil\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CFG_NAME_FOR_HYDRA = \"sam2_hiera_l.yaml\"  # Hydra expects the name, not the full path\n",
    "\n",
    "# If the config file isn’t packaged—copy it into the package directory once\n",
    "SAM2_PKG_DIR = os.path.dirname(sam2.__file__)\n",
    "PKG_CFG = os.path.join(SAM2_PKG_DIR, CFG_NAME_FOR_HYDRA)\n",
    "if (not os.path.exists(PKG_CFG)) and os.path.exists(CFG_PATH):\n",
    "    try:\n",
    "        shutil.copy(CFG_PATH, PKG_CFG)\n",
    "        print(\"Copied YAML into package:\", PKG_CFG)\n",
    "    except Exception as e:\n",
    "        print(\"Note:\", e)\n",
    "\n",
    "sam2_model = build_sam2(CFG_NAME_FOR_HYDRA, CKPT_PATH, device=device)\n",
    "mask_generator = SAM2AutomaticMaskGenerator(model=sam2_model, **SAM2_PARAMS)\n",
    "\n",
    "print(\"SAM2 loaded on\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, glob, csv, time, cv2, random, shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Collect files from all datasets in the list, recursively, for all relevant extensions\n",
    "PATTERNS = [\n",
    "    \"**/*.png\",\"**/*.jpg\",\"**/*.jpeg\",\"**/*.tif\",\"**/*.tiff\",\"**/*.bmp\",\n",
    "    \"**/*.PNG\",\"**/*.JPG\",\"**/*.JPEG\",\"**/*.TIF\",\"**/*.TIFF\",\"**/*.BMP\",\n",
    "]\n",
    "all_paths = []\n",
    "for base in INPUT_DIRS:\n",
    "    for pat in PATTERNS:\n",
    "        all_paths += glob.glob(os.path.join(base, pat), recursive=True)\n",
    "\n",
    "all_paths = sorted(set(all_paths))\n",
    "\n",
    "# Optional limit\n",
    "try:\n",
    "    if MAX_IMAGES is not None:\n",
    "        all_paths = all_paths[:MAX_IMAGES]\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Batch parameters\n",
    "BATCH_SIZE   = 100\n",
    "SHUFFLE      = False\n",
    "RESUME       = True\n",
    "\n",
    "# Which batch to start/stop from (1-based)\n",
    "START_BATCH  = 31\n",
    "END_BATCH    = None\n",
    "\n",
    "# Create ZIP for each batch upon completion (for download convenience)\n",
    "AUTO_ZIP_BATCH = True\n",
    "\n",
    "# How often to flush manifests (every N files)\n",
    "FLUSH_EVERY = 10\n",
    "\n",
    "if SHUFFLE:\n",
    "    random.seed(42); random.shuffle(all_paths)\n",
    "\n",
    "total = len(all_paths)\n",
    "num_batches = (total + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "start_b = max(START_BATCH - 1, 0)\n",
    "end_b   = num_batches if END_BATCH is None else min(END_BATCH, num_batches)\n",
    "\n",
    "print(f\"Found {total} images under {INPUT_DIRS}\")\n",
    "print(f\"Will run batches {start_b+1} .. {end_b} (of {num_batches})\")\n",
    "\n",
    "def open_manifest(path):\n",
    "    write_header = not (os.path.exists(path) and os.path.getsize(path) > 0)\n",
    "    f = open(path, \"a\" if not write_header else \"w\", newline=\"\", encoding=\"utf-8-sig\")\n",
    "    return f, write_header\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "root_manifest = os.path.join(DIRS[\"output\"], f\"manifest_{ts}.csv\")\n",
    "os.makedirs(DIRS[\"output\"], exist_ok=True)\n",
    "\n",
    "with open(root_manifest, \"w\", newline=\"\", encoding=\"utf-8-sig\") as mf:\n",
    "    writer = csv.writer(mf)\n",
    "    writer.writerow([\"batch_id\",\"idx_in_batch\",\"input_path_rel\",\"output_path_rel\",\"n_masks\",\"status\"])\n",
    "\n",
    "    for b in range(start_b, end_b):\n",
    "        batch_id   = f\"batch_{b+1:03d}\"\n",
    "        batch_dir  = os.path.join(DIRS[\"output\"], batch_id)\n",
    "        os.makedirs(batch_dir, exist_ok=True)\n",
    "\n",
    "        # per-batch manifest (append if exists)\n",
    "        batch_manifest = os.path.join(batch_dir, \"manifest.csv\")\n",
    "        bf, need_header = open_manifest(batch_manifest)\n",
    "        bwriter = csv.writer(bf)\n",
    "        if need_header:\n",
    "            bwriter.writerow([\"idx_in_batch\",\"input_path_rel\",\"output_path_rel\",\"n_masks\",\"status\"])\n",
    "\n",
    "        s = b * BATCH_SIZE\n",
    "        e = min(s + BATCH_SIZE, total)\n",
    "        batch_paths = all_paths[s:e]\n",
    "        print(f\"\\n=== {batch_id} ({s+1}-{e}/{total}) ===\")\n",
    "\n",
    "        for j, p in enumerate(batch_paths, 1):\n",
    "            base     = os.path.splitext(os.path.basename(p))[0]\n",
    "            out_path = os.path.join(batch_dir, f\"{base}_overlay.png\")\n",
    "\n",
    "            status, n_masks = \"ok\", \"\"\n",
    "            if RESUME and os.path.exists(out_path):\n",
    "                status = \"skipped_exists\"\n",
    "            else:\n",
    "                img_bgr = cv2.imread(p, cv2.IMREAD_UNCHANGED)\n",
    "                if img_bgr is None:\n",
    "                    status = \"read_error\"\n",
    "                else:\n",
    "                    try:\n",
    "                        img_rgb = to_rgb(img_bgr)\n",
    "                        h, w    = img_rgb.shape[:2]\n",
    "                        masks       = mask_generator.generate(img_rgb)\n",
    "                        n_masks     = len(masks)\n",
    "                        overlay_rgb = colorize_instances_sam2(masks, h, w)\n",
    "                        blended     = blend_overlay(img_rgb, overlay_rgb, alpha=ALPHA)\n",
    "                        cv2.imwrite(out_path, cv2.cvtColor(blended, cv2.COLOR_RGB2BGR))\n",
    "                    except Exception as e:\n",
    "                        status = f\"error:{type(e).__name__}\"\n",
    "                        n_masks = \"\"\n",
    "\n",
    "            rel_in  = os.path.relpath(p,        MAP_ROOT if 'MAP_ROOT' in globals() else DIRS[\"root\"])\n",
    "            rel_out = os.path.relpath(out_path, MAP_ROOT if 'MAP_ROOT' in globals() else DIRS[\"root\"])\n",
    "            row_root  = [batch_id, j, rel_in, rel_out, n_masks, status]\n",
    "            row_batch = [j,       rel_in, rel_out, n_masks, status]\n",
    "            writer.writerow(row_root)\n",
    "            bwriter.writerow(row_batch)\n",
    "\n",
    "            # small keep-alive output and manifest saving\n",
    "            if (j % FLUSH_EVERY) == 0 or j == len(batch_paths):\n",
    "                print(f\"  progress: {j}/{len(batch_paths)}\")\n",
    "                mf.flush(); bf.flush()\n",
    "\n",
    "        bf.close()\n",
    "\n",
    "        # Automatic ZIP for this batch\n",
    "        if AUTO_ZIP_BATCH:\n",
    "            zip_path = batch_dir + \".zip\"\n",
    "            try:\n",
    "                if os.path.exists(zip_path):\n",
    "                    os.remove(zip_path)  # refresh if files were added\n",
    "                shutil.make_archive(batch_dir, \"zip\", batch_dir)\n",
    "                print(f\"Zipped: {zip_path}\")\n",
    "            except Exception as e:\n",
    "                print(\"ZIP error:\", e)\n",
    "\n",
    "print(\"\\n✔️ Done.\")\n",
    "print(\"Root manifest:\", root_manifest)\n",
    "print(\"Per-batch manifests & ZIPs are under:\", DIRS[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !tar -C /kaggle -czf /kaggle/working/sem_sam2_output_$(date +%Y%m%d_%H%M%S).tgz working/SEM_SAM2/output\n",
    "# !ls -lh /kaggle/working | grep tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8365333,
     "sourceId": 13199652,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
