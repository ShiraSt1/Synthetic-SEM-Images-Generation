python - << 'PY'
import os, io, math
from pathlib import Path
from typing import List, Tuple
import torch
from PIL import Image
import requests

# =========================
# images
# =========================
SAMPLES: List[Tuple[str,str]] = [
    ("dog",      "https://images.unsplash.com/photo-1518717758536-85ae29035b6d?w=640"),   
#    ("wolf",     "https://images.unsplash.com/photo-1601758123927-196f0e1d6b0f?w=640"),  
    ("lion",     "https://images.unsplash.com/photo-1546182990-dffeafbe841d?w=640"),      
    ("cat",      "https://images.unsplash.com/photo-1592194996308-7b43878e84a6?w=640"),   
    ("car",      "https://images.unsplash.com/photo-1503376780353-7e6692767b70?w=640"),   
#    ("bus",      "https://images.unsplash.com/photo-1597007390957-41b5d2db5693?w=640"),  
    ("truck",    "https://images.unsplash.com/photo-1589571894960-20bbe2828d0a?w=640"),   
    ("pizza",    "https://images.unsplash.com/photo-1594007654729-407eedc4be65?w=640"),   
    ("burger",   "https://images.unsplash.com/photo-1550317138-10000687a72b?w=640"),      
    ("sushi",    "https://images.unsplash.com/photo-1553621042-f6e147245754?w=640"),      
    ("shoe",     "https://images.unsplash.com/photo-1491553895911-0055eca6402d?w=640"),   
    ("mountain", "https://images.unsplash.com/photo-1501785888041-af3ef285b470?w=640"),   
    ("laptop",   "https://images.unsplash.com/photo-1517336714731-489689fd1ca8?w=640"),   
    ("keyboard", "https://images.unsplash.com/photo-1517336714731-489689fd1ca8?w=640"),   
    ("diagram",  "https://raw.githubusercontent.com/openai/CLIP/main/CLIP.png"),          
]

# =========================
# labels
# =========================
LABELS = [
    "a photo of a dog", "a photo of a wolf", "a photo of a lion", "a photo of a cat", "a photo of a fox",
    "a photo of a car", "a photo of a bus", "a photo of a truck", "a photo of a bicycle",
    "a photo of a pizza", "a photo of a burger", "a photo of sushi",
    "a laptop computer", "a computer keyboard", "a smartphone", "a watch",
    "a building", "a shoe", "a mountain landscape",
    "a chair", "a book", "a cup of coffee",
    "a diagram", "an illustration", "a chart"
]

# =========================
# help functions
# =========================
def fetch_image(url: str) -> Image.Image:
    r = requests.get(url, timeout=15)
    r.raise_for_status()
    return Image.open(io.BytesIO(r.content)).convert("RGB")

def topk(probs, labels, k=5):
    pairs = list(zip(probs, labels))
    pairs.sort(key=lambda x: x[0], reverse=True)
    return pairs[:k]

def entropy_from_probs(probs):
    import math
    return -sum(p*math.log(p+1e-12) for p in probs)

def run_openai_clip(images, labels, device):
    import clip
    model, preprocess = clip.load("ViT-B/16", device=device)
    model.eval()
    with torch.no_grad():
        imgs = torch.stack([preprocess(im) for im in images]).to(device)
        txt  = clip.tokenize(labels).to(device)
        logits_img, _ = model(imgs, txt)
        probs = logits_img.softmax(dim=-1).cpu()
    return probs

def run_openclip(images, labels, device):
    import open_clip
    model, _, preprocess = open_clip.create_model_and_transforms(
        'ViT-B-16', pretrained='laion2b_s34b_b88k', device=device
    )
    tokenizer = open_clip.get_tokenizer('ViT-B-16')
    with torch.no_grad():
        imgs = torch.stack([preprocess(im) for im in images]).to(device)
        txt  = tokenizer(labels).to(device)
        img_f = model.encode_image(imgs)
        txt_f = model.encode_text(txt)
        logits = img_f @ txt_f.T
        probs = logits.softmax(dim=-1).cpu()
    return probs

# =========================
# running
# =========================
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Device: {device}")
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))

images, names = [], []
for name, url in SAMPLES:
    try:
        im = fetch_image(url)
    except Exception:
        print(f"Failed to fetch {name} -> {url}")
        im = Image.new("RGB", (640,420), (255,255,255))
    images.append(im)
    names.append(name)

BATCH = 2
oa_rows, oc_rows = [], []
for i in range(0, len(images), BATCH):
    batch = images[i:i+BATCH]
    oa_rows.append(run_openai_clip(batch, LABELS, device))
    oc_rows.append(run_openclip(batch, LABELS, device))

oa_all = torch.cat(oa_rows, dim=0).numpy().tolist()
oc_all = torch.cat(oc_rows, dim=0).numpy().tolist()

agree = []
for idx, name in enumerate(names):
    print("\n" + "="*100)
    print(f"Image #{idx+1}: {name}")
    print("-"*100)

    oa_probs = oa_all[idx]
    oa_top   = topk(oa_probs, LABELS, k=5)
    oa_sorted = sorted(oa_probs, reverse=True)
    oa_margin = oa_sorted[0] - oa_sorted[1]
    oa_H = entropy_from_probs(oa_probs)
    print("[OpenAI CLIP ViT-B/16]  Top-5:")
    for p, t in oa_top:
        print(f"  {t:35} p={p:.4f}")
    print(f"  Confidence margin (p1-p2): {oa_margin:.4f} | Entropy: {oa_H:.3f}")

    oc_probs = oc_all[idx]
    oc_top   = topk(oc_probs, LABELS, k=5)
    oc_sorted = sorted(oc_probs, reverse=True)
    oc_margin = oc_sorted[0] - oc_sorted[1]
    oc_H = entropy_from_probs(oc_probs)
    print("[OpenCLIP ViT-B/16 laion2b_s34b_b88k]  Top-5:")
    for p, t in oc_top:
        print(f"  {t:35} p={p:.4f}")
    print(f"  Confidence margin (p1-p2): {oc_margin:.4f} | Entropy: {oc_H:.3f}")

    oa_top1 = oa_top[0][1]
    oc_top1 = oc_top[0][1]
    same = (oa_top1 == oc_top1)
    agree.append(1 if same else 0)
    print(f"\nTop-1 agreement: {'YES' if same else 'NO'}  |  OpenAI: {oa_top1}  |  OpenCLIP: {oc_top1}")

if agree:
    rate = 100.0 * sum(agree) / len(agree)
    print("\n" + "="*100)
    print(f"Overall Top-1 agreement across {len(agree)} images: {rate:.1f}%")
    print("="*100)
PY
